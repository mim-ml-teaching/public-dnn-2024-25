{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDib219Y1e4z"
   },
   "source": [
    "# TL;DR\n",
    "\n",
    "In this lab scenario you will finish implementation of a variant of the Q-learning method, called DQN. On top of the usual q-learning using neural nets as function approximations, DQN uses:\n",
    "* experience replay - used to increase efficacy of samples from the environment and decorrelate elements of a batch, \n",
    "* target network - used to avoid constantly changing targets in the learning process (to avoid \"chasing own tail\").\n",
    "\n",
    "For algorithm's details recall the lecture and/or follow the [original paper](https://arxiv.org/abs/1312.5602), which is rather self-contained and not hard to understand. \n",
    "\n",
    "Without changing any hyperparameters, the agent should solve the problem (obtain rewards ~200) after ~1000 episodes, which for GPU runtime takes ~10 minutes of training.\n",
    "\n",
    "You can run this code locally (not in Colab), which allows to see the agent in action, unfortunately visualization inside Colab worked poorly and was removed from this lab scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbWvFo-wAP_N"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07GoFJCXC3li"
   },
   "outputs": [],
   "source": [
    "!pip install Box2D==2.3.10 gym==0.26.2 pygame==2.6.1\n",
    "!pip install numpy==1.26.4 torch\n",
    "!pip install tensorboard\n",
    "!pip install matplotlib\n",
    "!pip install ipywidgets\n",
    "!pip install opencv-python\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Le1nHKNN9_xa"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as display\n",
    "import ipywidgets as widgets\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Used to speed up the training in colab and avoid situations when the model is stuck\n",
    "# But this can be problematic for our model (why?)\n",
    "MAX_EPISODE_STEPS = 500 \n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quBickk6-Gxb"
   },
   "source": [
    "# Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2Y7aDTODOXq"
   },
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ptz2QtxJDPXw"
   },
   "outputs": [],
   "source": [
    "def try_gpu(i: int = 0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f\"cuda:{i}\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def save_model(model: nn.Module, PATH: str):\n",
    "    \"\"\"Saves model's state_dict.\n",
    "\n",
    "    Reference: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "def load_model(model: nn.Module, PATH: str):\n",
    "    \"\"\"Loads model's parameters from state_dict\"\"\"\n",
    "    model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWGKDXkY-U9C"
   },
   "source": [
    "## Scheduler\n",
    "\n",
    "Training RL agents requires dealing with exploration-exploitation trade-off. To handle this we will adopt the most basic, but extremely efficient, epsilon-greedy strategy. At the beginning our agent will focus on exploration, and over time will start exploiting his knowledge, and thus becoming more and more greedy. To implement this logic we will use LinearDecay scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Zl5GIWFJ-bsp"
   },
   "outputs": [],
   "source": [
    "class Constant:\n",
    "    \"\"\"Constant scheduler.\n",
    "\n",
    "    Can be used e.g. to create agent with with greedy policy, namely epsilon == 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value: float, *args):\n",
    "        self._value = value\n",
    "\n",
    "    def value(self, *args):\n",
    "        return self._value\n",
    "\n",
    "\n",
    "class LinearDecay:\n",
    "    \"\"\"Linear decay scheduler.\n",
    "\n",
    "    At each call linearly decays the value by simply subtracting `decay` from the current value,\n",
    "    until some minimum value is reached.\n",
    "    Can be used e.g. to decay epsilon value for epsilon-greedy exploration/exploitation strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, initial_value: float, final_value: float, decay: float):\n",
    "        self._value = initial_value\n",
    "        self.final_value = final_value\n",
    "        self.decay = decay\n",
    "\n",
    "    def value(self, *args) -> float:\n",
    "        self._value = max(self.final_value, self._value - self.decay)\n",
    "        return self._value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPds8uarDZUE"
   },
   "source": [
    "## Replay buffer\n",
    "\n",
    "The key trick that makes DQN feasible is replay buffer. The idea is to store observed transitions, sample them randomly and perform updates based on them. This solution has many advantages, the most significant ones are:\n",
    "\n",
    "1.   *Data efficiency* - each transition (env step) can be used in many weight updates.\n",
    "2.   *Data decorrelation* - consecutive transitions are naturally highly correlated. Randomizing the samples reduces these correlations, thus reducing variance of the updates.\n",
    "\n",
    "Note that when learning by experience replay, it is necessary to learn off-policy (because our current parameters are different to those used to generate the sample), which motivates the choice of Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oI62-4PtDb2v"
   },
   "outputs": [],
   "source": [
    "# non_terminal_mask is a mask indicating whether the state is terminal or not\n",
    "# it will become usefull when using target_net for predicting qvalues.\n",
    "Transition = namedtuple(\n",
    "    \"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"non_terminal_mask\")\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size: int):\n",
    "        \"\"\"Create new replay buffer.\n",
    "\n",
    "        Args:\n",
    "            size: capacity of the buffer\n",
    "        \"\"\"\n",
    "        self._storage: List[Transition] = []\n",
    "        self._capacity = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def add(self, data: Transition):\n",
    "        if len(self._storage) < self._capacity:\n",
    "            self._storage.append(None)\n",
    "        self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._capacity\n",
    "\n",
    "    def sample(self, batch_size: int) -> List[Transition]:\n",
    "        \"\"\"Sample batch of eixperience from memory.\n",
    "\n",
    "        Args:\n",
    "            batch_size: size of the batch\n",
    "\n",
    "        Returns:\n",
    "            batch of transitions\n",
    "        \"\"\"\n",
    "        batch = random.sample(self._storage, batch_size)\n",
    "        return batch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCQIj0ycAim0"
   },
   "source": [
    "## MLP Network\n",
    "\n",
    "For fast iteration we will stick to numerical observations (original DQN paper works with graphical observations). We will use simple MLP to net approximate our estimates of Q-values for (action, states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kBkWJjn-AglG"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP net.\n",
    "\n",
    "    Each of the layers, despite the last one, is followed by ReLU non-linearity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers_sizes: List[int]):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "        for in_features, out_features in zip(layers_sizes, layers_sizes[1:-1]):\n",
    "            modules.extend(\n",
    "                [\n",
    "                    nn.Linear(in_features, out_features),\n",
    "                    nn.ReLU(),\n",
    "                ]\n",
    "            )\n",
    "        # final output is not followed by non-linearity\n",
    "        modules.extend([nn.Linear(layers_sizes[-2], layers_sizes[-1])])\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.layers(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKqQYX7HBBOw"
   },
   "source": [
    "# DQN Agent\n",
    "\n",
    "First we implement constructor and some utility functions for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6C4yaDjB8FnV"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self, exploration_fn, policy_net: torch.nn.Module, target_net: torch.nn.Module\n",
    "    ):\n",
    "        self.exploration_fn = exploration_fn\n",
    "        self.policy_net = policy_net\n",
    "        self.target_net = target_net\n",
    "        self.optim = None\n",
    "        self.replay_buffer = None\n",
    "\n",
    "    def save_policy_net(self, checkpoint: str):\n",
    "        \"\"\"Saves policy_net parameters as given checkpoint.\n",
    "\n",
    "        state_dict of current policy_net is stored.\n",
    "\n",
    "        Args:\n",
    "            checkpoint: path were to store model's parameters.\n",
    "        \"\"\"\n",
    "        save_model(self.policy_net, checkpoint)\n",
    "\n",
    "    def load_policy_net(self, checkpoint: str):\n",
    "        \"\"\"Loads policy_net parameters from given checkpoint.\n",
    "\n",
    "        Note that proper model should be instantiated as only parameters of form state_dict\n",
    "        are stored as a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint: path to model's parameters.\n",
    "        \"\"\"\n",
    "        load_model(self.policy_net, checkpoint)\n",
    "\n",
    "    def play_episodes(self, n_episodes: int, env: gym.Env):\n",
    "        \"\"\"Function to watch the agent playing - locally\"\"\"\n",
    "\n",
    "        def render(frame, widget):\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            _, jpeg = cv2.imencode(\".jpeg\", frame)\n",
    "            widget.value = jpeg.tobytes()\n",
    "\n",
    "        self.policy_net.eval()\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            # 0.5 sec breaks between episodes, so it's easier to watch\n",
    "            time.sleep(0.5)\n",
    "            state = env.reset()\n",
    "\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "\n",
    "            total_reward, timesteps, done = 0, 0, False\n",
    "            image_widget = widgets.Image(format=\"jpeg\")\n",
    "            display.display(image_widget)\n",
    "            frame = env.render()\n",
    "            render(frame=frame, widget=image_widget)\n",
    "            \n",
    "            episode_steps_left = MAX_EPISODE_STEPS\n",
    "            while not done:\n",
    "                episode_steps_left -= 1\n",
    "                # Pick next action, simulate and observe next_state and reward\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _, _ = env.step(action.item())\n",
    "                done = done or episode_steps_left <= 0\n",
    "                state = next_state\n",
    "\n",
    "                frame = env.render()\n",
    "                render(frame=frame, widget=image_widget)\n",
    "                # To make watching easier\n",
    "                time.sleep(0.01)\n",
    "\n",
    "                total_reward += reward\n",
    "                timesteps += 1\n",
    "\n",
    "            print(f\"Episode length: {timesteps}, total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK0xexE68O7x"
   },
   "source": [
    "### Policy \n",
    "\n",
    "Given observation agent follows epsilon-greedy strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bs3kBCu68Mrk"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):\n",
    "    def act(self, obs) -> torch.Tensor:\n",
    "        \"\"\"Epsilon-greedy policy derived from policy_net\n",
    "\n",
    "        With probability epsilon select a random action a_t.\n",
    "        Otherwise select a_t = max_a(Q(obs, a; theta))\n",
    "        \"\"\"\n",
    "        eps_exploration = self.exploration_fn.value()\n",
    "        if torch.rand(1).item() <= eps_exploration:\n",
    "            return torch.randint(0, N_ACTIONS, [1])\n",
    "        else:\n",
    "            if not type(obs) == torch.Tensor:\n",
    "                obs = torch.tensor(obs, dtype=torch.float32, device=DEVICE).view(\n",
    "                    -1, OBS_SHAPE\n",
    "                )\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.policy_net(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wue8RcczBsaB"
   },
   "source": [
    "### Learning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uRVzkxNLBmQ3"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):\n",
    "    def learn(\n",
    "        self,\n",
    "        gamma: float,\n",
    "        optim: torch.optim.Optimizer,\n",
    "        n_episodes: int,\n",
    "        batch_size: int,\n",
    "        target_update_interval: int,\n",
    "        buffer_size: int,\n",
    "        checkpoints_dir: str,\n",
    "        checkpoint_save_interval: int,\n",
    "        tensorboard_log_dir: str,\n",
    "        env: gym.Env,\n",
    "    ):\n",
    "        self.optim = optim\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.policy_net.train()\n",
    "\n",
    "        total_steps, rewards_history = 0, []\n",
    "        writer = SummaryWriter(tensorboard_log_dir)\n",
    "\n",
    "        for episode in tqdm(range(n_episodes), desc=\"Training episode\"):\n",
    "            episode_reward, episode_steps, done = 0, 0, False\n",
    "            state = env.reset()\n",
    "\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "            episode_steps_left = MAX_EPISODE_STEPS\n",
    "            while not done:\n",
    "                episode_steps_left -= 1\n",
    "                # Pick next action, simulate and observe next_state and reward\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _, _ = env.step(action.item())\n",
    "                done = done or episode_steps_left <= 0\n",
    "\n",
    "                ##### TODO IMPLEMENT #####\n",
    "                # Store Transition in replay buffer. (\"state\", \"action\", \"next_state\", \"reward\", \"non_terminal_mask\")\n",
    "\n",
    "                ##### END OF TODO    #####\n",
    "\n",
    "                # Update target_net\n",
    "                loss = self._update_policy_net(gamma, batch_size)\n",
    "\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "\n",
    "                # Update target_net with current parameters\n",
    "                if (total_steps + 1) % target_update_interval == 0:\n",
    "                    self._update_target_net()\n",
    "                #\n",
    "                if (total_steps + 1) % checkpoint_save_interval == 0:\n",
    "                    self.save_policy_net(\n",
    "                        f\"{checkpoints_dir}/params_nsteps{total_steps + 1}_nepis{episode}\"\n",
    "                    )\n",
    "\n",
    "                # Misc\n",
    "                total_steps += 1\n",
    "                episode_steps += 1\n",
    "                episode_reward += reward\n",
    "                if loss:\n",
    "                    writer.add_scalar(\"Loss/MSE\", loss, total_steps)\n",
    "\n",
    "            rewards_history.append(episode_reward)\n",
    "            # Tensorboard\n",
    "            writer.add_scalar(\"Reward/episode\", episode_reward, episode)\n",
    "            writer.add_scalar(\n",
    "                \"Reward/mean_100_episodes\", np.mean(rewards_history[-100:]), episode\n",
    "            )\n",
    "            writer.add_scalar(\"Episode/n_steps\", episode_steps, episode)\n",
    "            writer.add_scalar(\n",
    "                \"Misc/eps_exploration\", self.exploration_fn._value, episode\n",
    "            )\n",
    "\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7cminMw81ko"
   },
   "source": [
    "### PolicyNet update step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "vEzYbq-hPpLb"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):\n",
    "    def _update_policy_net(self, gamma: float, batch_size: int):\n",
    "        \"\"\"Perform one round of policy_net update.\n",
    "\n",
    "        Sample random minibatch of transitions (fi(s_t), a_t, r_t, fi(s_t+1)) from replay buffer\n",
    "        and update policy_net according to DQN algorithm.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        def get_targets(gamma: float, batch: Transition):\n",
    "            \"\"\"Uses `target_net` and immediate rewards to calculate expected future rewards.\"\"\"\n",
    "            batch_next_state = torch.tensor(batch.next_state, device=DEVICE).detach()\n",
    "            # target_net prediction for terminal states should be 0, as our expectation from terminal state is 0\n",
    "            non_terminal_mask = torch.tensor(\n",
    "                batch.non_terminal_mask, device=DEVICE\n",
    "            ).detach()\n",
    "            next_state_bootstrapped_values = (\n",
    "                torch.max(self.target_net(batch_next_state), dim=1)[0].detach()\n",
    "                * non_terminal_mask\n",
    "            )\n",
    "            assert torch.all(\n",
    "                (non_terminal_mask == 0).nonzero()\n",
    "                == (next_state_bootstrapped_values == 0).nonzero()\n",
    "            )\n",
    "\n",
    "            assert len(batch.reward.shape) == 1\n",
    "            assert len(next_state_bootstrapped_values.shape) == 1\n",
    "            ##### TODO IMPLEMENT - given the pieces from above, compute the targets #####\n",
    "            # to match remaining portions of the code, reshape the target tensor as follows: (-1, 1)\n",
    "\n",
    "            # Expected future reward for terminal state is equal to immediate reward\n",
    "            # For non terminal states expected future reward:\n",
    "            # immediate reward + discounted future expectation\n",
    "            ##### END OF TODO    #####\n",
    "\n",
    "            assert targets.shape == (batch.next_state.shape[0], 1)\n",
    "            return targets\n",
    "\n",
    "        def get_state_action_values(batch):\n",
    "            \"\"\"Uses `policy_net` to calculate current estimates of future rewards.\"\"\"\n",
    "            batch_state = torch.tensor(batch.state, device=DEVICE)\n",
    "            # Calculate current estimates for the (state, action) we have observed and taken\n",
    "            # 'preds' shape: (batch_size, n_states, n_actions)\n",
    "            preds = self.policy_net(batch_state)\n",
    "            # Extracting values from various indices might be a little confusing:\n",
    "            # https://medium.com/analytics-vidhya/understanding-indexing-with-pytorch-gather-33717a84ebc4\n",
    "            action_index = torch.tensor(\n",
    "                batch.action, dtype=torch.long, device=DEVICE\n",
    "            ).unsqueeze(-1)\n",
    "            state_action_values = torch.gather(preds, dim=1, index=action_index)\n",
    "            return state_action_values\n",
    "\n",
    "        # Sample and convert batch into big Transition of form:\n",
    "        # Transition(state=(0,0,...), action=(1,4,...), next_state=(0,3,...), reward(3,0,...), non_terminal_mask(0,1,0,...))\n",
    "        # In other words: list_of_tuples -> tuple_of_lists\n",
    "        transitions = self.replay_buffer.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # Convert to numpy arrays so that we can use binary mask as indices to extract e.g. non terminal masks\n",
    "        # Types are chosen so that torch.tensor will inherit correct one\n",
    "        batch = Transition(\n",
    "            np.array(batch.state),\n",
    "            np.array(batch.action),\n",
    "            np.array(batch.next_state),\n",
    "            np.array(batch.reward, np.float32),\n",
    "            np.array(batch.non_terminal_mask, np.float32),\n",
    "        )\n",
    "\n",
    "        state_action_values = get_state_action_values(batch)\n",
    "        targets = get_targets(gamma, batch)\n",
    "        loss = F.mse_loss(state_action_values, targets)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3I1pWY5PzBa"
   },
   "source": [
    "### TargetNet update\n",
    "Finally, the last missing step is to *update target_net*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "H3JSgqcqPvCM"
   },
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):\n",
    "    def _update_target_net(self):\n",
    "        \"\"\"Sets `target_net` parameters to the current `policy_net` parameters.\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NM2rsPcFoQZ"
   },
   "source": [
    "# Environment\n",
    "\n",
    "We will try to solve: https://gym.openai.com/envs/LunarLander-v2/\n",
    "\n",
    "LunearLander env can be considered solved once we achieve 200 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1622586607956,
     "user": {
      "displayName": "Marek Cygan",
      "photoUrl": "",
      "userId": "15223245073200404878"
     },
     "user_tz": -120
    },
    "id": "4qxq_h-VFn-t",
    "outputId": "243f433a-6fdb-4179-e20f-9b09e1da9648"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "N_ACTIONS = env.action_space.n\n",
    "OBS_SHAPE = 8\n",
    "print(f\"Number of actions = {N_ACTIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCDEETSwZP4I"
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1622586619629,
     "user": {
      "displayName": "Marek Cygan",
      "photoUrl": "",
      "userId": "15223245073200404878"
     },
     "user_tz": -120
    },
    "id": "p3TKjP_iGPgV",
    "outputId": "4339e32e-b8ac-49fb-f879-3b7bdb168af8"
   },
   "outputs": [],
   "source": [
    "DEVICE = try_gpu()\n",
    "print(DEVICE)\n",
    "\n",
    "EXP_NAME = \"LunarLander\"\n",
    "LOG_DIR = f\"runs/{EXP_NAME}\"\n",
    "TENSORBOARD_LOG_DIR = f\"runs/{EXP_NAME}/tensorboard\"\n",
    "CHECKPOINTS_DIR = f\"runs/{EXP_NAME}/checkpoints\"\n",
    "Path(CHECKPOINTS_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "H7DI30GqKAQC"
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch DQN implementation\")\n",
    "\n",
    "    # Hack for colab...\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--fff\",\n",
    "        help=\"a dummy argument to fool ipython in colab. Comment out for local dev.\",\n",
    "        default=\"1\",\n",
    "    )\n",
    "\n",
    "    # To see the agent playing\n",
    "    parser.add_argument(\n",
    "        \"--play\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=\"play mode, if True then agent will play env instead of do training (default: False). \"\n",
    "        \"If checkpoint is not specified then randomly initialized network will play\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"checkpoint storing state_dict to load for the model. \"\n",
    "        \"If None then agent will be initialized with random params (default: None)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_episodes\", type=int, default=10, help=\"number of episodes to play\"\n",
    "    )\n",
    "\n",
    "    # To train the agent\n",
    "    parser.add_argument(\n",
    "        \"--exp_dir\",\n",
    "        type=str,\n",
    "        default=f\"exp/{datetime.datetime.now().timestamp()}\",\n",
    "        help=\"experiment directory were logs and checkpoints will be stored (default: exp/{timestamp}\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=14,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 5000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        type=float,\n",
    "        default=0.0005,\n",
    "        metavar=\"LR\",\n",
    "        help=\"learning rate (default: 0.0005)\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k9twCGQ59vnf"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    layers = [OBS_SHAPE, 256, 256, N_ACTIONS]\n",
    "    policy_net = MLP(layers).to(DEVICE)\n",
    "    target_net = MLP(layers).to(DEVICE)\n",
    "\n",
    "    agent_params = {\n",
    "        \"exploration_fn\": LinearDecay(1, 0.05, 0.00001),\n",
    "        \"policy_net\": policy_net,\n",
    "        \"target_net\": target_net,\n",
    "    }\n",
    "    if args.play:\n",
    "        print(\"Wanna play a game...\")\n",
    "        agent_params[\"exploration_fn\"] = Constant(0.01)\n",
    "        agent = DQNAgent(**agent_params)\n",
    "        if args.checkpoint:\n",
    "            agent.load_policy_net(args.checkpoint)\n",
    "        agent.play_episodes(args.n_episodes, env=env)\n",
    "\n",
    "    else:\n",
    "        print(\"Training mode...\")\n",
    "        train_params = {\n",
    "            \"gamma\": 0.99,\n",
    "            \"optim\": torch.optim.Adam(policy_net.parameters(), lr=0.0005),\n",
    "            \"n_episodes\": int(2e4),\n",
    "            \"batch_size\": 64,\n",
    "            # Target update interval in number of env steps (not episodes)\n",
    "            \"target_update_interval\": 100,\n",
    "            \"buffer_size\": 10000,\n",
    "            \"checkpoint_save_interval\": 5000,\n",
    "            \"checkpoints_dir\": CHECKPOINTS_DIR,\n",
    "            \"tensorboard_log_dir\": TENSORBOARD_LOG_DIR,\n",
    "        }\n",
    "\n",
    "        agent = DQNAgent(**agent_params)\n",
    "        agent.learn(**train_params, env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard in google colab\n",
    "# If you can't see anything run this cell twice\n",
    "%tensorboard --logdir $TENSORBOARD_LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwNTgAuKWEcj",
    "outputId": "916e9631-207e-4fa7-e13f-5676a8a9abd4"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUejGPxP5Bda"
   },
   "source": [
    "# Tasks\n",
    "\n",
    "\n",
    "1.   Implement missing code #### TODO IMPLEMENT #####\n",
    "2.   Experiment with the hyperparameters e.g. gamma (discount-factor), epsilon (for exploration-exploitation trade-off)\n",
    "3.   Observe weird behaviors of agent, e.g. \"forgetting how to play\" - reward going significantly down, and then \"re-learning\" again. Why can it happen? What can we do to avoid it?\n",
    "4.   Change the args and observe the trained model behavior. What do you see?\n",
    "5.   What can be improved in the training code?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GSN 2021/22 lab scenario - DQN - student version.ipynb",
   "provenance": [
    {
     "file_id": "1I4u4RXPxQFWzFfbZ7ThbMkuKbCe-geBy",
     "timestamp": 1622587264749
    },
    {
     "file_id": "1EEB6QXmYtvD5zEN2nhyW-3fqxO7NA7Gu",
     "timestamp": 1622532593636
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
