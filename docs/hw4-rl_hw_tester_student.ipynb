{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Testing Framework\n",
    "\n",
    "This notebook serves as an automated testing framework for homework assignment. Solution fields will be extracted from the designated notebook, `HW_NB_PATH`. Students can refer to the provided example inputs and outputs for better comprehension of the tasks. Commented print statements are included in the tests to aid debugging.\n",
    "\n",
    "## Homework Evaluation Criteria\n",
    "- **Final Evaluation**: Homeworks will be graded based on the same tests using different sets of inputs and outputs.\n",
    "- **Additional Checks**: Training plots displayed in the final cell of homework notebook and the overall training performance will also be reviewed.\n",
    "\n",
    "## Tester Notebook Requirements And Information\n",
    "- Use only provided context objects and variables for each solution (common: numpy, torch) - example: (numpy, torch, self, x, action)\n",
    "- See provided completed solution for reference which has its own test - `PPOT compute_advantages`.\n",
    "- Include your full name in the designated solution field **Student\\'s name** - **required**.\n",
    "- Use the designated solution field **Message** for any additional information to assist the reviewing instructor - optional.\n",
    "- You can modify this notebook because it is only for student aid - **do not send this testing notebook as a part of solved homework**.\n",
    "- In **homework notebook** do not modify any content outside the `### BEGIN SOLUTION ... ### END SOLUTION` blocks. If changes seem necessary, consult on Slack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eX70Cbpvm6hD"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import math\n",
    "import numpy\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "from functools import partial\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from gym.wrappers.normalize import RunningMeanStd\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# RL library for vectorized environments\n",
    "import pufferlib\n",
    "import pufferlib.vector\n",
    "import pufferlib.emulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sJYjwdIrm6hF"
   },
   "outputs": [],
   "source": [
    "HW_NB_PATH = \"hw4-ppo-rnd-minihack-student.ipynb\" # upload to colab env with homework notebook, or gather and run locally\n",
    "\n",
    "with open(HW_NB_PATH, \"r\") as f:\n",
    "    nb = json.load(f)\n",
    "cells = nb[\"cells\"]\n",
    "cells = [cell for cell in cells if cell[\"cell_type\"] == \"code\"]\n",
    "code = []\n",
    "for cell in cells:\n",
    "    code.append(\"\".join(cell[\"source\"]))\n",
    "code_all = \"\".join(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p7xSRY9xm6hG"
   },
   "outputs": [],
   "source": [
    "\n",
    "pattern = r\"( *### BEGIN SOLUTION - [\\s\\S]*?)### END SOLUTION\"\n",
    "matches = re.findall(pattern, code_all)\n",
    "\n",
    "solution = {\n",
    "    # \"example\":{\n",
    "    #     \"ix\": None,\n",
    "    #     \"content\": None,\n",
    "    #     \"code_indent\": None\n",
    "    # }\n",
    "}\n",
    "\n",
    "def count_leading_spaces(s):\n",
    "    match = re.match(r'^ *', s)\n",
    "    return len(match.group(0)) if match else 0\n",
    "\n",
    "def remove_indentation(solution_content, ls):\n",
    "    pattern = r'\\n {0,' + str(ls) + '}'\n",
    "    return re.sub(pattern, '\\n', solution_content)\n",
    "\n",
    "\n",
    "if matches:\n",
    "    for i, solution_content in enumerate(matches, start=1):\n",
    "        solution_name = solution_content.partition('\\n')[0]\n",
    "        ls = count_leading_spaces(solution_name)\n",
    "        solution_name = solution_name.replace(\" \"*ls + \"### BEGIN SOLUTION - \", \"\")\n",
    "        solution_content = \"\\n\".join(solution_content.partition('\\n')[1:])[1:]\n",
    "\n",
    "        solution_content = remove_indentation(solution_content, ls)\n",
    "        solution[solution_name] = {\n",
    "            \"ix\":i,\n",
    "            \"content\":solution_content,\n",
    "            \"code_indent\": ls\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sveP7HGVm6hH",
    "outputId": "0ec213ae-5614-45ba-f0e0-dafd6f3d8161"
   },
   "outputs": [],
   "source": [
    "print(solution.keys()) # should be 11 total - 2 info - 5 PPO - 4 RND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1VM4fuwm6hI",
    "outputId": "b510b9cd-8eaf-47d3-c102-ec8f91fe626c"
   },
   "outputs": [],
   "source": [
    "def student_info(solution):\n",
    "    assert \"\\n# --- --------\\n\" != solution['Student\\'s name']['content'], \"Please provide name and surname\"\n",
    "    print(f\"Name:\\n\" + solution['Student\\'s name']['content'])\n",
    "    print(f\"Message:\\n\" + solution['Message']['content'])\n",
    "student_info(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "copgzTbFm6hJ"
   },
   "source": [
    "## Argument Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r7x1cQJJm6hJ"
   },
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if isinstance(v, str) and v.lower() in (\"true\",):\n",
    "        return True\n",
    "    elif isinstance(v, str) and v.lower() in (\"false\",):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(\"Boolean value expected\")\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = \"ppo_rnd\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: str2bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: str2bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: str2bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: str2bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"MiniHack-KeyRoom-Dark-S5-v0\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    max_episode_steps: int = None\n",
    "    \"\"\"number of episode steps\"\"\"\n",
    "    total_timesteps: int = 5000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 2e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 128\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_workers: int = 16\n",
    "    \"\"\"the number of workers\"\"\"\n",
    "    num_steps: int = 128\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: str2bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.999\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 8\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 8\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: str2bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.1\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: str2bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.001\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 40\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "    penalty_step: float = 0.0\n",
    "    \"\"\"the penalty for each env step\"\"\"\n",
    "    value_bootstrap: str2bool = True\n",
    "    \"\"\"Value bootstrapping\"\"\"\n",
    "\n",
    "    # RND arguments\n",
    "    update_proportion: float = 1.0\n",
    "    \"\"\"proportion of exp used for predictor update\"\"\"\n",
    "    int_coef: float = 0.1\n",
    "    \"\"\"coefficient of extrinsic reward\"\"\"\n",
    "    ext_coef: float = 1.0\n",
    "    \"\"\"coefficient of intrinsic reward\"\"\"\n",
    "    int_gamma: float = 0.99\n",
    "    \"\"\"Intrinsic reward discount rate\"\"\"\n",
    "    num_iterations_obs_norm_init: int = 5\n",
    "    \"\"\"number of iterations to initialize the observations normalization parameters\"\"\"\n",
    "    forward_coef: float = 0.1\n",
    "    \"\"\"weight on modelling loss (ie convergence of predictor)\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTRCFQq8m6hK"
   },
   "source": [
    "## RecordEpisodeStatistics, RewardForwardFilter, layer_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E5eg8C3jm6hK"
   },
   "outputs": [],
   "source": [
    "class RecordEpisodeStatistics:\n",
    "    def __init__(self, env, deque_size=100):\n",
    "        self.env = env\n",
    "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
    "        self.episode_returns = None\n",
    "        self.episode_lengths = None\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observations, infos = self.env.reset(**kwargs)\n",
    "        self.episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.lives = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.returned_episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.returned_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        return observations, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        observations, rewards, terminatedes, truncatedes, infos = self.env.step(action)\n",
    "        self.episode_returns += rewards\n",
    "        self.episode_lengths += 1\n",
    "        self.returned_episode_returns[:] = self.episode_returns\n",
    "        self.returned_episode_lengths[:] = self.episode_lengths\n",
    "        self.episode_returns *= 1 - terminatedes\n",
    "        self.episode_lengths *= 1 - terminatedes\n",
    "\n",
    "        infos = {}\n",
    "        infos[\"r\"] = self.returned_episode_returns\n",
    "        infos[\"l\"] = self.returned_episode_lengths\n",
    "\n",
    "        return (\n",
    "            observations,\n",
    "            rewards,\n",
    "            terminatedes,\n",
    "            truncatedes,\n",
    "            infos,\n",
    "        )\n",
    "\n",
    "\n",
    "class RewardForwardFilter:\n",
    "    def __init__(self, gamma):\n",
    "        self.rewems = None\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, rews):\n",
    "        if self.rewems is None:\n",
    "            self.rewems = rews\n",
    "        else:\n",
    "            self.rewems = self.rewems * self.gamma + rews\n",
    "        return self.rewems\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwaAXIEHm6hL"
   },
   "source": [
    "## Create Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IQrrU4oHm6hL"
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from nle.env.base import NLE\n",
    "\n",
    "class NLETimeLimit(gymnasium.Wrapper):\n",
    "    def __init__(self, env: gymnasium.Env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "        if \"end_status\" in info:  # Add safety check\n",
    "            if info[\"end_status\"] == NLE.StepStatus.ABORTED:\n",
    "                truncated = True\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "def env_creator(args):\n",
    "    '''NetHack binding creation function'''\n",
    "    import minihack\n",
    "    import gym\n",
    "    import shimmy\n",
    "    from pufferlib.environments.minihack.environment import MinihackWrapper, EXTRA_OBS_KEYS\n",
    "\n",
    "    obs_key = minihack.base.MH_DEFAULT_OBS_KEYS + EXTRA_OBS_KEYS\n",
    "    kwargs = dict(\n",
    "        observation_keys=obs_key,\n",
    "        penalty_step=args.penalty_step,\n",
    "    )\n",
    "    env = gym.make(args.env_id, **kwargs)\n",
    "    env = shimmy.GymV21CompatibilityV0(env=env)\n",
    "    env = NLETimeLimit(env)\n",
    "    env = MinihackWrapper(env)\n",
    "    return pufferlib.emulation.GymnasiumPufferEnv(env=env)\n",
    "\n",
    "def create_env(args):\n",
    "    env = pufferlib.vector.make(partial(env_creator, args), batch_size=args.num_envs, num_envs=args.num_envs, num_workers=args.num_workers, backend=pufferlib.vector.Multiprocessing)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NXNNemoIm6hM"
   },
   "outputs": [],
   "source": [
    "seed = 27\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "global_common_context = {\"torch\":torch, \"np\":np, \"numpy\":numpy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWgJ74sXm6hM"
   },
   "source": [
    "# PPO Agent - Solution context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uIE-x-Qgm6hN"
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.dtype = pufferlib.pytorch.nativize_dtype(env.emulated)\n",
    "\n",
    "        self.blstats_net = nn.Sequential(\n",
    "            nn.Embedding(256, 32),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.char_embed = nn.Embedding(256, 32)\n",
    "        self.chars_net = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(32, 32, 5, stride=(2, 3))),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 5, stride=(1, 3))),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.proj = nn.Linear(864+960, 256)\n",
    "        self.actor = layer_init(nn.Linear(256, env.single_action_space.n), std=0.01)\n",
    "        self.critic_ext = layer_init(nn.Linear(256, 1), std=0.01)\n",
    "        self.critic_int = layer_init(nn.Linear(256, 1), std=0.01)\n",
    "\n",
    "    def encode_observations(self, x):\n",
    "        x = x.type(torch.uint8) # Undo bad cleanrl cast\n",
    "        x = pufferlib.pytorch.nativize_tensor(x, self.dtype)\n",
    "\n",
    "        blstats = torch.clip(x['blstats'] + 1, 0, 255).int()\n",
    "        blstats = self.blstats_net(blstats)\n",
    "\n",
    "        chars = self.char_embed(x['chars'].int())\n",
    "        chars = torch.permute(chars, (0, 3, 1, 2))\n",
    "        chars = self.chars_net(chars)\n",
    "\n",
    "        concat = torch.cat([blstats, chars], dim=1)\n",
    "        return self.proj(concat)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None, student_solution=None):\n",
    "        ### BEGIN SOLUTION - PPO get_action_and_value\n",
    "        input_context = {\"self\": self, \"x\": x, \"action\": action}\n",
    "        global global_common_context\n",
    "        input_context.update(global_common_context)\n",
    "        output_context = {}\n",
    "        exec(student_solution, input_context, output_context)\n",
    "        action = output_context.get(\"action\")\n",
    "        log_prob = output_context.get(\"log_prob\")\n",
    "        entropy_prop = output_context.get(\"entropy_prop\")\n",
    "        extrinsic_output = output_context.get(\"extrinsic_output\")\n",
    "        intrinsic_output = output_context.get(\"intrinsic_output\")\n",
    "        ### END SOLUTION\n",
    "        return (\n",
    "            action,\n",
    "            log_prob,\n",
    "            entropy_prop,\n",
    "            extrinsic_output,\n",
    "            intrinsic_output\n",
    "        )\n",
    "\n",
    "    def get_value(self, x, student_solution:str):\n",
    "        ### BEGIN SOLUTION - PPO get_value\n",
    "        input_context = {\"self\": self, \"x\": x}\n",
    "        global global_common_context\n",
    "        input_context.update(global_common_context)\n",
    "        output_context = {}\n",
    "        exec(student_solution, input_context, output_context)\n",
    "        extrinsic_output = output_context.get(\"extrinsic_output\")\n",
    "        intrinsic_output = output_context.get(\"intrinsic_output\")\n",
    "        ### END SOLUTION\n",
    "        return extrinsic_output, intrinsic_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "raMGATNBm6hN"
   },
   "outputs": [],
   "source": [
    "args = Args(\n",
    "    num_steps=32,\n",
    "    num_workers=8,\n",
    "    num_envs=128,\n",
    "    env_id=\"MiniHack-KeyRoom-S5-v0\",\n",
    "    total_timesteps=100000,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "env = create_env(args)\n",
    "\n",
    "agent = Agent(env).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv_lkUEfm6hN",
    "outputId": "20583dfe-a4be-44a8-ee2e-14c9ebec82be"
   },
   "outputs": [],
   "source": [
    "exp_PPO_get_action_and_value = {\n",
    "    \"x\":torch.ones([1, 13016]).to(device),\n",
    "    \"action\":None,\n",
    "    'entropy_prop':2.3025617599487305,\n",
    "    'extrinsic_output':-0.0010622446425259113,\n",
    "    'intrinsic_output':0.0011777263134717941\n",
    "}\n",
    "\n",
    "def test_PPO_get_action_and_value(agent, solution, exp_in_out: dict, rel_tol=1e-1):\n",
    "    action, log_prob, entropy_prop, extrinsic_output, intrinsic_output = agent.get_action_and_value(exp_in_out['x'], action=exp_in_out['action'], student_solution = solution['PPO get_action_and_value']['content'])\n",
    "    # print(f\"action: {action.item()}\") #dev\n",
    "    # print(f\"log_prob: {log_prob.item()}\")\n",
    "    # print(f\"entropy_prop: {entropy_prop.item()}\")\n",
    "    # print(f\"extrinsic_output: {extrinsic_output.item()}\")\n",
    "    # print(f\"intrinsic_output: {intrinsic_output.item()}\")\n",
    "\n",
    "    assert math.isclose(entropy_prop.item(), exp_in_out['entropy_prop'], rel_tol=rel_tol)\n",
    "    assert math.isclose(extrinsic_output.item(), exp_in_out['extrinsic_output'], rel_tol=rel_tol)\n",
    "    assert math.isclose(intrinsic_output.item(), exp_in_out['intrinsic_output'], rel_tol=rel_tol)\n",
    "\n",
    "print(solution['PPO get_action_and_value']['content']) #dev\n",
    "test_PPO_get_action_and_value(agent, solution, exp_PPO_get_action_and_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBYO6QNUm6hO",
    "outputId": "c56ae9f3-a5d0-4ad2-9b92-312015e4aa72"
   },
   "outputs": [],
   "source": [
    "exp_PPO_get_value = {\n",
    "    \"x\":torch.ones([128, 13016]).to(device),\n",
    "    'extrinsic_output[0][0]':-0.001062246854417026,\n",
    "    'intrinsic_output[0][0]':0.0011777239851653576\n",
    "}\n",
    "\n",
    "def test_PPO_get_value(agent, solution, exp_in_out: dict, rel_tol=1e-2):\n",
    "    extrinsic_output, intrinsic_output = agent.get_value(exp_in_out['x'], solution['PPO get_value']['content'])\n",
    "    # print(extrinsic_output.shape) #dev\n",
    "    # print(extrinsic_output[0][0].item())\n",
    "    # print(intrinsic_output.shape)\n",
    "    # print(intrinsic_output[0][0].item())\n",
    "    assert math.isclose(extrinsic_output[0][0].item(), exp_in_out['extrinsic_output[0][0]'], rel_tol=rel_tol)\n",
    "    assert math.isclose(intrinsic_output[0][0].item(), exp_in_out['intrinsic_output[0][0]'], rel_tol=rel_tol)\n",
    "\n",
    "print(solution['PPO get_value']['content']) #dev\n",
    "test_PPO_get_value(agent, solution, exp_PPO_get_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqFjkcJ1m6hO"
   },
   "source": [
    "# PPO Trainer - Solution context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "B20XzEtpm6hO"
   },
   "outputs": [],
   "source": [
    "class PPOTrainer:\n",
    "    def __init__(self, args, env, agent):\n",
    "        self.args = args\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.device = next(agent.parameters()).device\n",
    "\n",
    "        self.combined_parameters = list(self.agent.parameters())\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.combined_parameters,\n",
    "            lr=self.args.learning_rate,\n",
    "            eps=1e-5,\n",
    "        )\n",
    "\n",
    "        self.obs = torch.zeros((self.args.num_steps, self.args.num_envs) + self.env.single_observation_space.shape).to(self.device)\n",
    "        self.actions = torch.zeros((self.args.num_steps, self.args.num_envs) + self.env.single_action_space.shape).to(self.device)\n",
    "        self.logprobs = torch.zeros((self.args.num_steps, self.args.num_envs)).to(self.device)\n",
    "        self.rewards = torch.zeros((self.args.num_steps, self.args.num_envs)).to(self.device)\n",
    "        self.dones = torch.zeros((self.args.num_steps, self.args.num_envs)).to(self.device)\n",
    "        self.time_outs = torch.zeros((self.args.num_steps, self.args.num_envs)).to(self.device)\n",
    "        self.ext_values = torch.zeros((self.args.num_steps, self.args.num_envs)).to(self.device)\n",
    "\n",
    "        self.er_counter = 0\n",
    "        self.av_er_interval = 20\n",
    "\n",
    "    def compute_advantages(self, next_obs, next_done, student_solution:str, student_solution_ppo_get_value:str):\n",
    "        with torch.no_grad():\n",
    "            next_value_ext, _ = self.agent.get_value(next_obs, student_solution_ppo_get_value)\n",
    "            # tip - next_obs not needed in solution context :)\n",
    "            ### BEGIN SOLUTION - PPOT compute_advantages\n",
    "            input_context = {\"self\": self, \"next_obs\": next_obs, \"next_done\": next_done, \"next_value_ext\":next_value_ext}\n",
    "            global global_common_context\n",
    "            input_context.update(global_common_context)\n",
    "            output_context = {}\n",
    "\n",
    "            exec(student_solution, input_context, output_context)\n",
    "            ext_advantages = output_context.get(\"ext_advantages\")\n",
    "            ### END SOLUTION\n",
    "        return ext_advantages\n",
    "\n",
    "    def compute_policy_loss(self, mb_advantages, ratio, student_solution:str):\n",
    "        ### BEGIN SOLUTION - PPOT compute_policy_loss\n",
    "        input_context = {\"self\": self, \"mb_advantages\": mb_advantages, \"ratio\": ratio}\n",
    "        global global_common_context\n",
    "        input_context.update(global_common_context)\n",
    "        output_context = {}\n",
    "\n",
    "        exec(student_solution, input_context, output_context)\n",
    "        pg_loss = output_context.get(\"pg_loss\")\n",
    "        ### END SOLUTION\n",
    "        return pg_loss\n",
    "\n",
    "    def compute_value_loss(self, new_values, returns, student_solution:str):\n",
    "        ### BEGIN SOLUTION - PPOT compute_value_loss\n",
    "        input_context = {\"self\": self, \"new_values\": new_values, \"returns\": returns}\n",
    "        global global_common_context\n",
    "        input_context.update(global_common_context)\n",
    "        output_context = {}\n",
    "\n",
    "        exec(student_solution, input_context, output_context)\n",
    "        v_loss = output_context.get(\"v_loss\")\n",
    "        ### END SOLUTION\n",
    "        return v_loss\n",
    "\n",
    "    def train(self):\n",
    "        # ALGO Logic: Storage setup\n",
    "        avg_returns = deque(maxlen=20)\n",
    "\n",
    "        # start the game\n",
    "        global_step = 0\n",
    "        start_time = time.time()\n",
    "        next_obs = torch.Tensor(self.env.reset()[0]).to(self.device)\n",
    "        next_done = torch.zeros(self.args.num_envs).to(self.device)\n",
    "        next_time_out = torch.zeros(self.args.num_envs).to(self.device)\n",
    "        num_updates = self.args.total_timesteps // self.args.batch_size\n",
    "\n",
    "        for update in range(1, num_updates + 1):\n",
    "            # Annealing the rate if instructed to do so.\n",
    "            if self.args.anneal_lr:\n",
    "                frac = 1.0 - (update - 1.0) / num_updates\n",
    "                lrnow = frac * self.args.learning_rate\n",
    "                self.optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "            for step in range(0, self.args.num_steps):\n",
    "                global_step += 1 * self.args.num_envs\n",
    "                self.obs[step] = next_obs\n",
    "                self.dones[step] = next_done\n",
    "                self.time_outs[step] = next_time_out\n",
    "\n",
    "                # ALGO LOGIC: action logic\n",
    "                with torch.no_grad():\n",
    "                    value_ext, _ = self.agent.get_value(self.obs[step])\n",
    "                    self.ext_values[step] = value_ext.flatten()\n",
    "                    action, logprob, _, _, _ = self.agent.get_action_and_value(self.obs[step])\n",
    "\n",
    "                self.actions[step] = action\n",
    "                self.logprobs[step] = logprob\n",
    "\n",
    "                # execute the game and log data.\n",
    "                next_obs, reward, terminated, truncated, info = self.env.step(action.cpu().numpy())\n",
    "                self.rewards[step] = torch.tensor(reward).to(self.device).view(-1)\n",
    "                next_obs = torch.Tensor(next_obs).to(self.device)\n",
    "                next_done = torch.Tensor(terminated).to(self.device)\n",
    "                next_time_out = torch.Tensor(truncated).to(self.device)\n",
    "\n",
    "                for idx, d in enumerate(terminated | truncated):\n",
    "                    if d:\n",
    "                        avg_returns.append(info[\"r\"][idx])\n",
    "                        epi_ret = np.average(avg_returns)\n",
    "                        epi_suc = np.average([1 if r > 0 else 0 for r in avg_returns])\n",
    "\n",
    "            if self.args.value_bootstrap:\n",
    "                self.rewards.add_(self.args.gamma * self.ext_values * self.time_outs)\n",
    "\n",
    "            # bootstrap value if not done\n",
    "            with torch.no_grad():\n",
    "                ext_advantages = self.compute_advantages(next_obs, next_done)\n",
    "                ext_returns = ext_advantages + self.ext_values\n",
    "\n",
    "            # flatten the batch\n",
    "            b_obs = self.obs.reshape((-1,) + self.env.single_observation_space.shape)\n",
    "            b_logprobs = self.logprobs.reshape(-1)\n",
    "            b_actions = self.actions.reshape(-1)\n",
    "            b_ext_advantages = ext_advantages.reshape(-1)\n",
    "            b_ext_returns = ext_returns.reshape(-1)\n",
    "\n",
    "            b_advantages = b_ext_advantages * self.args.ext_coef\n",
    "\n",
    "            # Optimizing the policy and value network\n",
    "            b_inds = np.arange(self.args.batch_size)\n",
    "\n",
    "            clipfracs = []\n",
    "            for epoch in range(self.args.update_epochs):\n",
    "                np.random.shuffle(b_inds)\n",
    "                for start in range(0, self.args.batch_size, self.args.minibatch_size):\n",
    "                    end = start + self.args.minibatch_size\n",
    "                    mb_inds = b_inds[start:end]\n",
    "\n",
    "                    _, newlogprob, entropy, new_ext_values, _ = self.agent.get_action_and_value(\n",
    "                        b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "                    )\n",
    "                    logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                    ratio = logratio.exp()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                        old_approx_kl = (-logratio).mean()\n",
    "                        approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                        clipfracs += [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]\n",
    "\n",
    "                    mb_advantages = b_advantages[mb_inds]\n",
    "                    if self.args.norm_adv:\n",
    "                        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                    # Policy loss\n",
    "                    pg_loss = self.compute_policy_loss(mb_advantages, ratio)\n",
    "\n",
    "                    # Value loss\n",
    "                    v_loss = self.compute_value_loss(new_ext_values, b_ext_returns[mb_inds])\n",
    "\n",
    "                    # Entropy loss\n",
    "                    entropy_loss = entropy.mean()\n",
    "                    loss = pg_loss - self.args.ent_coef * entropy_loss + v_loss * self.args.vf_coef\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    if self.args.max_grad_norm:\n",
    "                        nn.utils.clip_grad_norm_(\n",
    "                            self.combined_parameters,\n",
    "                            self.args.max_grad_norm,\n",
    "                        )\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                if self.args.target_kl is not None:\n",
    "                    if approx_kl > self.args.target_kl:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fpiY1qUhm6hO"
   },
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(args, env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPXKaYOym6hO",
    "outputId": "820f69eb-3517-4e00-9d31-911441996887"
   },
   "outputs": [],
   "source": [
    "exp_PPOT_compute_advantages = {\n",
    "    'next_obs':torch.ones([128, 13016]).to(device),\n",
    "    'next_done':torch.zeros([128]).to(device),\n",
    "    'ext_advantages[0][0]':-0.0002,\n",
    "    'ext_advantages.shape':[32, 128],\n",
    "}\n",
    "\n",
    "\n",
    "def test_PPOT_compute_advantages(trainer:PPOTrainer, solution, exp_in_out: dict, rel_tol=1e-1):\n",
    "    ext_advantages = trainer.compute_advantages(exp_in_out['next_obs'], exp_in_out['next_done'], solution['PPOT compute_advantages']['content'], solution['PPO get_value']['content'])\n",
    "    # print(ext_advantages[:1]) #dev\n",
    "    # print(ext_advantages[0][0])\n",
    "    # print(ext_advantages.shape)\n",
    "\n",
    "    assert list(ext_advantages.shape) == exp_in_out['ext_advantages.shape']\n",
    "    assert math.isclose(ext_advantages[0][0].item(), exp_in_out['ext_advantages[0][0]'], rel_tol=rel_tol)\n",
    "\n",
    "print(solution['PPOT compute_advantages']['content']) #dev\n",
    "test_PPOT_compute_advantages(trainer, solution, exp_PPOT_compute_advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlQipCF5m6hP",
    "outputId": "d1bbfa2c-702c-4980-cbcc-c60e9e7a4f0d"
   },
   "outputs": [],
   "source": [
    "ratio_input = torch.ones([512]).to(device)\n",
    "ratio_input[0:10] = 10\n",
    "exp_PPOT_compute_policy_loss = {\n",
    "    'mb_advantages':torch.ones([512]).to(device),\n",
    "    'ratio':ratio_input,\n",
    "    'pg_loss':-1.001953125,\n",
    "}\n",
    "\n",
    "def test_PPOT_compute_policy_loss(trainer:PPOTrainer, solution, exp_in_out: dict, rel_tol=1e-6):\n",
    "    pg_loss = trainer.compute_policy_loss(exp_in_out['mb_advantages'], exp_in_out['ratio'], solution['PPOT compute_policy_loss']['content'])\n",
    "    # print(pg_loss.item()) #dev\n",
    "    assert math.isclose(pg_loss.item(), exp_in_out['pg_loss'], rel_tol=rel_tol)\n",
    "\n",
    "print(solution['PPOT compute_policy_loss']['content']) #dev\n",
    "test_PPOT_compute_policy_loss(trainer, solution, exp_PPOT_compute_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D64iMO6Qm6hP",
    "outputId": "d2c0fca5-d5b5-406d-def2-d579b94efcf8"
   },
   "outputs": [],
   "source": [
    "returns_input = torch.ones([512]).to(device)\n",
    "returns_input[0:10] = 10\n",
    "exp_PPOT_compute_value_loss = {\n",
    "    'new_values':torch.ones([512, 1]).to(device),\n",
    "    'returns':returns_input,\n",
    "    'v_loss': 0.791015625,\n",
    "}\n",
    "\n",
    "def test_PPOT_compute_value_loss(trainer:PPOTrainer, solution, exp_in_out: dict, rel_tol=1e-4):\n",
    "    v_loss = trainer.compute_value_loss(exp_in_out['new_values'], exp_in_out['returns'], solution['PPOT compute_value_loss']['content'])\n",
    "    # print(v_loss.item()) #dev\n",
    "\n",
    "    assert math.isclose(v_loss.item(), exp_in_out['v_loss'], rel_tol=rel_tol)\n",
    "\n",
    "print(solution['PPOT compute_value_loss']['content']) #dev\n",
    "test_PPOT_compute_value_loss(trainer, solution, exp_PPOT_compute_value_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a7reBOtm6hP"
   },
   "source": [
    "# RND Model - Solution context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "r8toBT5cm6hP"
   },
   "outputs": [],
   "source": [
    "class RNDModel(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dtype = pufferlib.pytorch.nativize_dtype(env.emulated)\n",
    "\n",
    "        # Predictor Network\n",
    "        self.predictor_blstats_net = nn.Sequential(\n",
    "            nn.Embedding(256, 32),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.predictor_char_embed = nn.Embedding(256, 32)\n",
    "        self.predictor_chars_net = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(32, 32, 5, stride=(2, 3))),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 5, stride=(1, 3))),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.predictor_proj = nn.Linear(864+960, 256)\n",
    "        self.predictor = nn.Sequential(\n",
    "            layer_init(nn.Linear(256, 512)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(512, 512)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(512, 512)),\n",
    "        )\n",
    "\n",
    "        # Target Network\n",
    "        self.target_blstats_net = nn.Sequential(\n",
    "            nn.Embedding(256, 32),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.target_char_embed = nn.Embedding(256, 32)\n",
    "        self.target_chars_net = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(32, 32, 5, stride=(2, 3))),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(32, 64, 5, stride=(1, 3))),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.target_proj = nn.Linear(864+960, 256)\n",
    "        self.target = nn.Sequential(\n",
    "            layer_init(nn.Linear(256, 512)),\n",
    "        )\n",
    "\n",
    "        # target network is not trainable\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.startswith('target'):\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def encode_predictor(self, x):\n",
    "        x = x.type(torch.uint8)  # Undo bad cleanrl cast\n",
    "        x = pufferlib.pytorch.nativize_tensor(x, self.dtype)\n",
    "\n",
    "        blstats = torch.clip(x['blstats'] + 1, 0, 255).int()\n",
    "        blstats = self.predictor_blstats_net(blstats)\n",
    "\n",
    "        chars = self.predictor_char_embed(x['chars'].int())\n",
    "        chars = torch.permute(chars, (0, 3, 1, 2))\n",
    "        chars = self.predictor_chars_net(chars)\n",
    "\n",
    "        concat = torch.cat([blstats, chars], dim=1)\n",
    "        return self.predictor_proj(concat)\n",
    "\n",
    "    def encode_target(self, x):\n",
    "        x = x.type(torch.uint8)  # Undo bad cleanrl cast\n",
    "        x = pufferlib.pytorch.nativize_tensor(x, self.dtype)\n",
    "\n",
    "        blstats = torch.clip(x['blstats'] + 1, 0, 255).int()\n",
    "        blstats = self.target_blstats_net(blstats)\n",
    "\n",
    "        chars = self.target_char_embed(x['chars'].int())\n",
    "        chars = torch.permute(chars, (0, 3, 1, 2))\n",
    "        chars = self.target_chars_net(chars)\n",
    "\n",
    "        concat = torch.cat([blstats, chars], dim=1)\n",
    "        return self.target_proj(concat)\n",
    "\n",
    "    def forward(self, next_obs, student_solution:str):\n",
    "        ### BEGIN SOLUTION - RND forward\n",
    "        input_context = {\"self\": self, \"next_obs\": next_obs}\n",
    "        global global_common_context\n",
    "        input_context.update(global_common_context)\n",
    "        output_context = {}\n",
    "\n",
    "        exec(student_solution, input_context, output_context)\n",
    "        predict_feature = output_context.get(\"predict_feature\")\n",
    "        target_feature = output_context.get(\"target_feature\")\n",
    "        ### END SOLUTION\n",
    "        return predict_feature, target_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eQ_lHW2xm6hP"
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "args = Args(\n",
    "    num_steps=32,\n",
    "    num_workers=8,\n",
    "    num_envs=128,\n",
    "    env_id=\"MiniHack-KeyRoom-S5-v0\",\n",
    "    total_timesteps=100000,\n",
    "    forward_coef=0.1,\n",
    "    int_coef=0.1,\n",
    "    num_iterations_obs_norm_init=50,\n",
    ")\n",
    "\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "args.num_iterations = args.total_timesteps // args.batch_size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "env = create_env(args)\n",
    "\n",
    "rnd_model = RNDModel(env).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4ClJsG7m6hQ",
    "outputId": "47626659-7d88-4995-88e9-34af24b7fbc5"
   },
   "outputs": [],
   "source": [
    "exp_RND_forward = {\n",
    "    'next_obs': torch.zeros([128, 13016]).to(device),\n",
    "    'predict_feature[0][1]': 0.02161688357591629,\n",
    "    'predict_feature.shape': [128, 512],\n",
    "    'target_feature[0][1]': 0.4340367019176483,\n",
    "    'target_feature.shape': [128, 512],\n",
    "}\n",
    "\n",
    "def test_PPOT_compute_value_loss(rnd_model:RNDModel, solution, exp_in_out: dict, rel_tol=1e-2):\n",
    "    predict_feature, target_feature = rnd_model.forward(exp_in_out['next_obs'], solution['RND forward']['content'])\n",
    "    # print(predict_feature.shape)  #dev\n",
    "    # print(predict_feature[0][1].item())\n",
    "    # print(predict_feature)\n",
    "    # print(target_feature.shape)\n",
    "    # print(target_feature[0][1].item())\n",
    "    # print(target_feature)\n",
    "\n",
    "    assert exp_in_out['predict_feature.shape'] == list(predict_feature.shape)\n",
    "    assert exp_in_out['target_feature.shape'] == list(target_feature.shape)\n",
    "    assert math.isclose(predict_feature[0][1].item(), exp_in_out['predict_feature[0][1]'], rel_tol=rel_tol)\n",
    "    assert math.isclose(target_feature[0][1].item(), exp_in_out['target_feature[0][1]'], rel_tol=rel_tol)\n",
    "\n",
    "print(solution['RND forward']['content']) #dev\n",
    "test_PPOT_compute_value_loss(rnd_model, solution, exp_RND_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yI3zsPPtm6hQ"
   },
   "source": [
    "# RND Trainer - solution context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TuWGBPahm6hQ"
   },
   "outputs": [],
   "source": [
    "class RNDTrainer(PPOTrainer):\n",
    "    def __init__(self, args, env, agent, rnd_model):\n",
    "        super().__init__(args, env, agent)\n",
    "        self.rnd_model = rnd_model\n",
    "        self.curiosity_rewards = torch.zeros((self.args.num_steps, self.args.num_envs)).to(self.device)\n",
    "        self.int_values = torch.zeros((self.args.num_steps, self.args.num_envs)).to(self.device)\n",
    "\n",
    "        self.combined_parameters = list(self.agent.parameters()) + list(self.rnd_model.predictor.parameters())\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.combined_parameters,\n",
    "            lr=self.args.learning_rate,\n",
    "            eps=1e-5,\n",
    "        )\n",
    "\n",
    "    def calculate_curiosity_rewards(self, step, next_obs, student_solution:str):\n",
    "        ### BEGIN SOLUTION - RNDT calculate_curiosity_rewards\n",
    "        input_context = {\"self\": self, \"step\": step, \"next_obs\": next_obs}\n",
    "        global global_common_context\n",
    "        input_context.update(global_common_context)\n",
    "        output_context = {}\n",
    "\n",
    "        exec(student_solution, input_context, output_context)\n",
    "        steps_curiosity_rewards = output_context.get(\"steps_curiosity_rewards\")\n",
    "        ### END SOLUTION\n",
    "        self.curiosity_rewards[step] = steps_curiosity_rewards.to(self.device)\n",
    "        return steps_curiosity_rewards # returned for tests\n",
    "\n",
    "\n",
    "    def compute_int_advantages(self, next_obs, student_solution:str):\n",
    "        with torch.no_grad():\n",
    "            ### BEGIN SOLUTION - RNDT compute_int_advantages\n",
    "            input_context = {\"self\": self, \"next_obs\": next_obs}\n",
    "            global global_common_context\n",
    "            input_context.update(global_common_context)\n",
    "            output_context = {}\n",
    "\n",
    "            exec(student_solution, input_context, output_context)\n",
    "            int_advantages = output_context.get(\"int_advantages\")\n",
    "            ### END SOLUTION\n",
    "            return int_advantages\n",
    "\n",
    "    def compute_rnd_loss(self, rnd_next_obs, student_solution:str):\n",
    "        ### BEGIN SOLUTION - RNDT compute_rnd_loss\n",
    "        input_context = {\"self\": self, \"rnd_next_obs\": rnd_next_obs}\n",
    "        global global_common_context\n",
    "        input_context.update(global_common_context)\n",
    "        output_context = {}\n",
    "\n",
    "        exec(student_solution, input_context, output_context)\n",
    "        forward_loss = output_context.get(\"forward_loss\")\n",
    "        ### END SOLUTION\n",
    "        return forward_loss\n",
    "\n",
    "    def train(self):\n",
    "        # ALGO Logic: Storage setup\n",
    "        avg_returns = deque(maxlen=20)\n",
    "\n",
    "        # start the game\n",
    "        global_step = 0\n",
    "        start_time = time.time()\n",
    "        next_obs = torch.Tensor(self.env.reset()[0]).to(self.device)\n",
    "        next_done = torch.zeros(self.args.num_envs).to(self.device)\n",
    "        next_time_out = torch.zeros(self.args.num_envs).to(self.device)\n",
    "        num_updates = self.args.total_timesteps // self.args.batch_size\n",
    "\n",
    "        reward_rms = RunningMeanStd()\n",
    "        self.obs_rms = RunningMeanStd(shape=(1,) + self.env.single_observation_space.shape)\n",
    "        discounted_reward = RewardForwardFilter(self.args.int_gamma)\n",
    "\n",
    "        print(\"Start to initialize observation normalization parameter.....\")\n",
    "        next_ob = []\n",
    "        for step in range(self.args.num_steps * self.args.num_iterations_obs_norm_init):\n",
    "            acs = np.random.randint(0, self.env.single_action_space.n, size=(self.args.num_envs,))\n",
    "            o, r, te, tr, _ = self.env.step(acs)\n",
    "            next_ob += o.tolist()\n",
    "\n",
    "            if len(next_ob) % (self.args.num_steps * self.args.num_envs) == 0:\n",
    "                next_ob = np.stack(next_ob)\n",
    "                self.obs_rms.update(next_ob)\n",
    "                next_ob = []\n",
    "        print(\"End to initialize...\")\n",
    "\n",
    "        for update in range(1, num_updates + 1):\n",
    "            # Annealing the rate if instructed to do so.\n",
    "            if self.args.anneal_lr:\n",
    "                frac = 1.0 - (update - 1.0) / num_updates\n",
    "                lrnow = frac * self.args.learning_rate\n",
    "                self.optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "            for step in range(0, self.args.num_steps):\n",
    "                global_step += 1 * self.args.num_envs\n",
    "                self.obs[step] = next_obs\n",
    "                self.dones[step] = next_done\n",
    "                self.time_outs[step] = next_time_out\n",
    "\n",
    "                # ALGO LOGIC: action logic\n",
    "                with torch.no_grad():\n",
    "                    value_ext, value_int = self.agent.get_value(self.obs[step])\n",
    "                    self.ext_values[step], self.int_values[step] = (\n",
    "                        value_ext.flatten(),\n",
    "                        value_int.flatten(),\n",
    "                    )\n",
    "                    action, logprob, _, _, _ = self.agent.get_action_and_value(self.obs[step])\n",
    "\n",
    "                self.actions[step] = action\n",
    "                self.logprobs[step] = logprob\n",
    "\n",
    "                # execute the game and log data.\n",
    "                next_obs, reward, terminated, truncated, info = self.env.step(action.cpu().numpy())\n",
    "                self.rewards[step] = torch.tensor(reward).to(self.device).view(-1)\n",
    "                next_obs = torch.Tensor(next_obs).to(self.device)\n",
    "                next_done = torch.Tensor(terminated).to(self.device)\n",
    "                next_time_out = torch.Tensor(truncated).to(self.device)\n",
    "\n",
    "                self.calculate_curiosity_rewards(step, next_obs)\n",
    "\n",
    "                for idx, d in enumerate(terminated | truncated):\n",
    "                    if d:\n",
    "                        avg_returns.append(info[\"r\"][idx])\n",
    "                        epi_ret = np.average(avg_returns)\n",
    "                        epi_suc = np.average([1 if r > 0 else 0 for r in avg_returns])\n",
    "\n",
    "            curiosity_reward_per_env = np.array(\n",
    "                [discounted_reward.update(reward_per_step) for reward_per_step in self.curiosity_rewards.cpu().data.numpy().T]\n",
    "            )\n",
    "            mean, std, count = (\n",
    "                np.mean(curiosity_reward_per_env),\n",
    "                np.std(curiosity_reward_per_env),\n",
    "                len(curiosity_reward_per_env),\n",
    "            )\n",
    "            reward_rms.update_from_moments(mean, std**2, count)\n",
    "\n",
    "            self.curiosity_rewards /= np.sqrt(reward_rms.var)\n",
    "\n",
    "            if self.args.value_bootstrap:\n",
    "                # Value bootstrapping is a technique that reduces the surprise for the critic in case\n",
    "                # we're ending the episode by timeout. Intuitively, in this case the cumulative return for the last step\n",
    "                # should not be zero, but rather what the critic expects. This improves learning in many envs\n",
    "                # because otherwise the critic cannot predict the abrupt change in rewards in a timed-out episode.\n",
    "                # What we really want here is v(t+1) which we don't have because we don't have obs(t+1) (since\n",
    "                # the episode ended). Using v(t) is an approximation that requires that rew(t) can be generally ignored.\n",
    "                self.rewards.add_(self.args.gamma * self.ext_values * self.time_outs)\n",
    "                self.curiosity_rewards.add_(self.args.int_gamma * self.int_values * self.time_outs)\n",
    "\n",
    "            # bootstrap value if not done\n",
    "            with torch.no_grad():\n",
    "                ext_advantages = self.compute_advantages(next_obs, next_done)\n",
    "                int_advantages = self.compute_int_advantages(next_obs)\n",
    "                ext_returns = ext_advantages + self.ext_values\n",
    "                int_returns = int_advantages + self.int_values\n",
    "\n",
    "            # flatten the batch\n",
    "            b_obs = self.obs.reshape((-1,) + self.env.single_observation_space.shape)\n",
    "            b_logprobs = self.logprobs.reshape(-1)\n",
    "            b_actions = self.actions.reshape(-1)\n",
    "            b_ext_advantages = ext_advantages.reshape(-1)\n",
    "            b_int_advantages = int_advantages.reshape(-1)\n",
    "            b_ext_returns = ext_returns.reshape(-1)\n",
    "            b_int_returns = int_returns.reshape(-1)\n",
    "            b_ext_values = self.ext_values.reshape(-1)\n",
    "\n",
    "            b_advantages = b_int_advantages * self.args.int_coef + b_ext_advantages * self.args.ext_coef\n",
    "\n",
    "            self.obs_rms.update(b_obs.cpu().numpy())\n",
    "\n",
    "            # Optimizing the policy and value network\n",
    "            b_inds = np.arange(self.args.batch_size)\n",
    "\n",
    "            rnd_next_obs = (\n",
    "                (\n",
    "                    (b_obs - torch.from_numpy(self.obs_rms.mean).to(self.device))\n",
    "                    / torch.sqrt(torch.from_numpy(self.obs_rms.var).to(self.device))\n",
    "                ).clip(-5, 5)\n",
    "            ).float()\n",
    "\n",
    "            clipfracs = []\n",
    "            for epoch in range(self.args.update_epochs):\n",
    "                np.random.shuffle(b_inds)\n",
    "                for start in range(0, self.args.batch_size, self.args.minibatch_size):\n",
    "                    end = start + self.args.minibatch_size\n",
    "                    mb_inds = b_inds[start:end]\n",
    "\n",
    "                    # Forward loss\n",
    "                    forward_loss = self.compute_rnd_loss(rnd_next_obs[mb_inds])\n",
    "\n",
    "                    _, newlogprob, entropy, new_ext_values, new_int_values = self.agent.get_action_and_value(\n",
    "                        b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "                    )\n",
    "                    logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                    ratio = logratio.exp()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                        old_approx_kl = (-logratio).mean()\n",
    "                        approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                        clipfracs += [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]\n",
    "\n",
    "                    mb_advantages = b_advantages[mb_inds]\n",
    "                    if self.args.norm_adv:\n",
    "                        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                    # Policy loss\n",
    "                    pg_loss = self.compute_policy_loss(mb_advantages, ratio)\n",
    "\n",
    "                    # Value loss\n",
    "                    ext_v_loss = self.compute_value_loss(new_ext_values, b_ext_returns[mb_inds])\n",
    "                    int_v_loss = self.compute_value_loss(new_int_values, b_int_returns[mb_inds])\n",
    "                    v_loss = ext_v_loss + int_v_loss\n",
    "\n",
    "                    # Entropy loss\n",
    "                    entropy_loss = entropy.mean()\n",
    "                    loss = pg_loss - self.args.ent_coef * entropy_loss + v_loss * self.args.vf_coef + self.args.forward_coef * forward_loss\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    if self.args.max_grad_norm:\n",
    "                        nn.utils.clip_grad_norm_(\n",
    "                            self.combined_parameters,\n",
    "                            self.args.max_grad_norm,\n",
    "                        )\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                if self.args.target_kl is not None:\n",
    "                    if approx_kl > self.args.target_kl:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "izQqA1a1m6hR"
   },
   "outputs": [],
   "source": [
    "trainer = RNDTrainer(args, env, agent, rnd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBdOgoc1m6hR",
    "outputId": "0994add5-4cc6-429f-bdf6-bd4898e3dfaf"
   },
   "outputs": [],
   "source": [
    "\n",
    "exp_RNDT_calculate_curiosity_rewards = {\n",
    "    'seed':27,\n",
    "    'trainer.rnd_model = lambda x:': [128],\n",
    "    'step': 0,\n",
    "    'next_obs': torch.ones([128, 13016]).to(device),\n",
    "    'steps_curiosity_rewards[0]': 41.92801284790039,\n",
    "    'steps_curiosity_rewards[1]': 43.05669403076172,\n",
    "    'steps_curiosity_rewards.shape': [128],\n",
    "}\n",
    "\n",
    "\n",
    "def test_RNDT_calculate_curiosity_rewards(trainer:RNDTrainer, solution, exp_in_out: dict, rel_tol=1e-4):\n",
    "    torch.manual_seed(exp_in_out['seed'])\n",
    "    trainer.obs_rms = RunningMeanStd(shape=(1,) + trainer.env.single_observation_space.shape)\n",
    "    trainer.rnd_model = lambda _: (torch.rand([128, 512]).to(device), torch.rand([128, 512]).to(device))\n",
    "    steps_curiosity_rewards = trainer.calculate_curiosity_rewards(exp_in_out['step'], exp_in_out['next_obs'], solution['RNDT calculate_curiosity_rewards']['content'])\n",
    "    # print(steps_curiosity_rewards.shape)  #dev\n",
    "    # print(steps_curiosity_rewards[0].item())\n",
    "    # print(steps_curiosity_rewards[1].item())\n",
    "\n",
    "    assert exp_in_out['steps_curiosity_rewards.shape'] == list(steps_curiosity_rewards.shape)\n",
    "    assert math.isclose(steps_curiosity_rewards[0].item(), exp_in_out['steps_curiosity_rewards[0]'], rel_tol=rel_tol)\n",
    "    assert math.isclose(steps_curiosity_rewards[1].item(), exp_in_out['steps_curiosity_rewards[1]'], rel_tol=rel_tol)\n",
    "\n",
    "print(solution['RNDT calculate_curiosity_rewards']['content']) #dev\n",
    "test_RNDT_calculate_curiosity_rewards(trainer, solution, exp_RNDT_calculate_curiosity_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LN1w5TgOm6hR",
    "outputId": "7742fa75-74b5-4638-c4ad-ba476e50dbef"
   },
   "outputs": [],
   "source": [
    "exp_RNDT_compute_int_advantages = {\n",
    "    'next_obs': torch.ones([128, 13016]).to(device), # passed to self.agent.get_value - return replaced with lambda below\n",
    "    'int_advantages[0][0]': 41.92818832397461,\n",
    "    'int_advantages[0][1]': 43.05686950683594,\n",
    "    'int_advantages[2][0]': 0.00019682712445501238,\n",
    "    'int_advantages[2][1]': 0.00019682712445501238,\n",
    "    'int_advantages.shape': [32, 128],\n",
    "    'trainer.agent.get_value = lambda x:': (torch.tensor(-0.0010622446425259113).to(device), torch.tensor(0.0011777263134717941).to(device)),\n",
    "}\n",
    "\n",
    "def test_RNDT_compute_int_advantages(trainer:RNDTrainer, solution, exp_in_out: dict, rel_tol=1e-4):\n",
    "    trainer.agent.get_value = lambda _: exp_in_out['trainer.agent.get_value = lambda x:']\n",
    "    int_advantages = trainer.compute_int_advantages(exp_in_out['next_obs'], solution['RNDT compute_int_advantages']['content'])\n",
    "    # print(int_advantages.shape)  #dev\n",
    "    # print(int_advantages[0][0].item())\n",
    "    # print(int_advantages[0][1].item())\n",
    "    # print(int_advantages[2][0].item())\n",
    "    # print(int_advantages[2][1].item())\n",
    "\n",
    "    assert exp_in_out['int_advantages.shape'] == list(int_advantages.shape)\n",
    "    assert math.isclose(int_advantages[0][0].item(), exp_in_out['int_advantages[0][0]'], rel_tol=rel_tol)\n",
    "    assert math.isclose(int_advantages[0][1].item(), exp_in_out['int_advantages[0][1]'], rel_tol=rel_tol)\n",
    "    assert math.isclose(int_advantages[2][0].item(), exp_in_out['int_advantages[2][0]'], rel_tol=rel_tol)\n",
    "    assert math.isclose(int_advantages[2][1].item(), exp_in_out['int_advantages[2][1]'], rel_tol=rel_tol)\n",
    "\n",
    "print(solution['RNDT compute_int_advantages']['content']) #dev\n",
    "test_RNDT_compute_int_advantages(trainer, solution, exp_RNDT_compute_int_advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idLqBNufm6hR",
    "outputId": "fef96ba7-065e-4c5d-9280-fccef26571bf"
   },
   "outputs": [],
   "source": [
    "exp_RNDT_compute_rnd_loss = {\n",
    "    'seed': 27,\n",
    "    'rnd_next_obs': torch.ones([128, 13016]).to(device), # passed to self.agent.get_value - return replaced with lambda below\n",
    "    'forward_loss': 0.1669679880142212,\n",
    "}\n",
    "\n",
    "def test_RNDT_compute_rnd_loss(trainer:RNDTrainer, solution, exp_in_out: dict, rel_tol=1e-3):\n",
    "    torch.manual_seed(exp_RNDT_compute_rnd_loss['seed'])\n",
    "    trainer.rnd_model = lambda _: (torch.rand([128, 512]).to(device), torch.rand([128, 512]).to(device))\n",
    "    forward_loss = trainer.compute_rnd_loss(exp_in_out['rnd_next_obs'], solution['RNDT compute_rnd_loss']['content'])\n",
    "    # print(forward_loss.item())  #dev\n",
    "\n",
    "    assert math.isclose(forward_loss.item(), exp_in_out['forward_loss'], rel_tol=rel_tol)\n",
    "\n",
    "print(solution['RNDT compute_rnd_loss']['content']) #dev\n",
    "test_RNDT_compute_rnd_loss(trainer, solution, exp_RNDT_compute_rnd_loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
